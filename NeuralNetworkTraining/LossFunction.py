import numpy as np

# 신경망이 학습할 때, 손실 함수를 사용한다.
# 손실 함수는 훈련 결과 (신경망이 추정한 값) 과 정답을 비교하여
# 그에 맞는 오차를 알려준다.
# 훈련 결과가 정답에 가까울 수록 손실 함수의 출력은 줄어들고, 정답에서 멀어질 수록 함수의 출력은 크다.
# 손실 함수의 출력은 즉 얼마나 틀렸는가, 얼마나 잘못된 값인가를 나타낸다.
# 손실 함수로는 평균 제곱 오차, 교차 엔트로피 오차를 많이 쓴다.


# 평균 제곱 오차는 출력과 정답의 차의 제곱을 모두 더한 뒤 반으로 나눈다.
def meanSquaredError(y, t) : # y 는 신경망의 출력, t 는 정답
    return np.sum((y-t)**2)/2

t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])  # 정답은 2 이다.
y = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])  # 학습 결과 2일 확률이 0.6이라고 판단하였다. (소프트맥스 출력)
print(meanSquaredError(y, t))
# 결과는 0.0975.. 이다.

y = np.array([0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]) # 7 일 확률이 0.6 이라고 하였다.
print(meanSquaredError(y, t))
# 결과는 0.5975 이다. 이렇듯 틀리면 틀릴 수록 값이 커진다.
# 이 손실 함수를 이용하여 신경망 학습에 필요한 가중치를 수정해나갈 수 있다. (자동화)


# 교차 엔트로피 오차는 출력에 자연로그를 취한 뒤 정답을 곱하고 모두 더한 뒤 음수를 취한다.
def crossEntropyError(y, t) :
    delta = 1e-7 # 0이 들어와서 log 값이 무한대가 되는 것을 방지한다.
    return -np.sum(t*np.log(y+delta))

print(crossEntropyError(y, t))
# 7 일 확률이 높게 출력되면 2.3025.. 라는 값을 출력한다.

y = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])  # 학습 결과 2일 확률이 0.6이라고 판단하였다. (소프트맥스 출력)
print(crossEntropyError(y, t))
# 이는 0.5108.. 을 출력한다. 평균 제곱 오차와 같이 정답일 수록 값이 줄고 오답일 수록 값이 는다.
